{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Road to Data Science in 50 Days - Day 5\n",
    "***\n",
    "\n",
    "## Statistics for Data Science\n",
    "\n",
    "![](https://hackr.io/blog/statistics-for-data-science/thumbnail/large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's 2 more part of math that is usually used in Data Science i.e. Linear Algebra and Bayesian Statistics.\n",
    "\n",
    "After thourougly going through various articles on Linear Algebra, i have realized that linear algebra would be pretty difficult to learn from written blogs and it needs video explanations. So we will get introduced to the basics of Linear Algebra and suggest some videos to go through in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra\n",
    "\n",
    "In linear algebra, you will often hear the terms vectors, matrix or tensors in deep learning. Let's get introduced to these terms:\n",
    "![](https://miro.medium.com/max/700/1*pUr-9ctuGamgjSwoW_KU-A.png)\n",
    "\n",
    "## 1. Scalar:  \n",
    "A scalar is usually said to be a physical quantity that only has magnitude, possibly a sign, and no other characteristics\n",
    "\n",
    "## 2. Vector: \n",
    "![](https://www.mathsisfun.com/algebra/images/vector.gif)\n",
    "\n",
    "A vector has magnitude (size) and direction\n",
    "![](https://www.mathsisfun.com/algebra/images/vector-mag-dir.svg)\n",
    "The length of the line shows its magnitude and the arrowhead points in the direction.\n",
    "\n",
    "### Addition of 2 Vectors\n",
    "We can add two vectors by joining them head-to-tail:\n",
    "![](https://www.mathsisfun.com/algebra/images/vector-add.svg)\n",
    "\n",
    "And it doesn't matter which order we add them, we get the same result:\n",
    "![](https://www.mathsisfun.com/algebra/images/vector-add2.gif)\n",
    "\n",
    "We can also subtract one vector from another:\n",
    "\n",
    "* first we reverse the direction of the vector we want to subtract,\n",
    "* then add them as usual:\n",
    "![](https://www.mathsisfun.com/algebra/images/vector-subtract.gif)\n",
    "\n",
    "### Magnitude of a Vector\n",
    "The magnitude of a vector is shown by two vertical bars on either side of the vector:\n",
    "\n",
    "    |a|\n",
    "\n",
    "OR it can be written with double vertical bars (so as not to confuse it with absolute value):\n",
    "\n",
    "    ||a||\n",
    "\n",
    "We use Pythagoras' theorem to calculate it:\n",
    "\n",
    "    |a| = √( x2 + y2 )\n",
    "    \n",
    "#### Example:\n",
    "\n",
    "what is the magnitude of the vector b = (6, 8) ?\n",
    "\n",
    "    |b| = √( 62 + 82) = √( 36+64) = √100 = 10\n",
    "\n",
    "A vector with magnitude 1 is called a <a href=\"https://www.mathsisfun.com/algebra/vector-unit.html\">Unit Vector</a>\n",
    "\n",
    "**To learn more about vectors in details, refer: https://www.mathsisfun.com/algebra/vectors.html\n",
    "\n",
    "\n",
    "## 3. A Matrix is an array of numbers\n",
    "![](https://www.mathsisfun.com/algebra/images/matrix-example.svg)\n",
    "(This one has 2 Rows and 3 Columns)\n",
    "\n",
    "There are many things we can do with them ...\n",
    "\n",
    "### Adding\n",
    "\n",
    "To add two matrices: add the numbers in the matching positions:\n",
    "![](https://www.mathsisfun.com/algebra/images/matrix-addition.gif)\n",
    "\n",
    "These are the calculations:\n",
    "\n",
    "    3+4=7\t8+0=8\n",
    "    \n",
    "    4+1=5\t6−9=−3\n",
    "    \n",
    "The two matrices must be the same size, i.e. the rows must match in size, and the columns must match in size.\n",
    "\n",
    "\n",
    "### Negative\n",
    "\n",
    "The negative of a matrix is also simple:\n",
    "![](https://www.mathsisfun.com/algebra/images/matrix-negative.gif)\n",
    "\n",
    "Matrix Negative\n",
    "\n",
    "These are the calculations:\n",
    "\n",
    "    −(2)=−2\t−(−4)=+4\n",
    "\n",
    "    −(7)=−7\t−(10)=−10\n",
    "\n",
    "###  Subtracting\n",
    "\n",
    "To subtract two matrices: subtract the numbers in the matching positions:\n",
    "![](https://www.mathsisfun.com/algebra/images/matrix-subtraction.gif)\n",
    "Matrix Subtraction\n",
    "\n",
    "These are the calculations:\n",
    "\n",
    "    3−4=−1\t8−0=8\n",
    "    \n",
    "    4−1=3\t6−(−9)=15\n",
    "\n",
    "### Multiply by a Constant\n",
    "\n",
    "We can multiply a matrix by a constant (the value 2 in this case):\n",
    "\n",
    "![](https://www.mathsisfun.com/algebra/images/matrix-multiply-constant.gif)\n",
    "\n",
    "\n",
    "These are the calculations:\n",
    "\n",
    "    2×4=8\t2×0=0\n",
    "    \n",
    "    2×1=2\t2×−9=−18\n",
    "    \n",
    "We call the constant a scalar, so officially this is called \"**scalar multiplication**\".\n",
    "\n",
    "### Multiplying by Another Matrix\n",
    "\n",
    "![](https://miro.medium.com/max/427/1*96qrPHcvXBVM01I1lUKS8g.png)\n",
    "\n",
    "To multiply two matrices together is a bit more difficult ... read <a href=\"https://www.mathsisfun.com/algebra/matrix-multiplying.html\">Multiplying Matrices</a> to learn how.\n",
    "\n",
    "### Dividing\n",
    "\n",
    "And what about division? Well we don't actually divide matrices, we do it this way:\n",
    "\n",
    "    A/B = A × (1/B) = A × B-1\n",
    "\n",
    "    where B-1 means the \"inverse\" of B.\n",
    "\n",
    "So we don't divide, instead we multiply by an inverse.\n",
    "\n",
    "And there are special ways to find the Inverse, learn more at <a href=\"https://www.mathsisfun.com/algebra/matrix-inverse.html\">Inverse of a Matrix</a>.\n",
    "\n",
    "### Transposing\n",
    "\n",
    "To \"transpose\" a matrix, swap the rows and columns.\n",
    "![](https://www.mathsisfun.com/algebra/images/matrix-transpose.gif)\n",
    "\n",
    "We put a \"T\" in the top right-hand corner to mean transpose\n",
    "\n",
    "### Types of Matrix\n",
    "\n",
    "![](https://www.mathsisfun.com/algebra/images/matrix-main-diagonal.svg)\n",
    "\n",
    "There are various types of matrix such as:\n",
    "1. Diagonal Matrix\n",
    "2. Inverse Matrix\n",
    "3. Square Matrix\n",
    "4. Identity Matrix\n",
    "5. Scalar Matrix\n",
    "6. Triangular Matrix\n",
    "7. Symmetric\n",
    "8. Hermitian\n",
    "\n",
    "To learn about the types of matrix in details:, refer https://www.mathsisfun.com/algebra/matrix-types.html\n",
    "\n",
    "\n",
    "### Notation\n",
    "\n",
    "A matrix is usually shown by a capital letter (such as A, or B)\n",
    "\n",
    "Each entry (or \"element\") is shown by a lower case letter with a \"subscript\" of row,column:\n",
    "![](https://www.mathsisfun.com/algebra/images/matrix-notation.gif)\n",
    "\n",
    "\n",
    "Rows and Columns\n",
    "So which is the row and which is the column?\n",
    "\n",
    "* Rows go left-right\n",
    "* Columns go up-down\n",
    "\n",
    "To remember that rows come before columns use the word \"arc\":\n",
    "\n",
    "Ar,c\n",
    "\n",
    "\n",
    "## 4. Tensors\n",
    "\n",
    "Well tensors are a bit too much to learn at the basic, i would suggest learn and practise more and more on vectors and matrices calculations before starting with Tensors.\n",
    "\n",
    "Tensors can be defined in multiple ways:\n",
    "\n",
    "1. **Abstract way**: they are mathematical objects, that has certain properties, and transforms in a particular way.\n",
    "\n",
    "\n",
    "2. **Algebraic way**: they are a generalization of the usual 2D matrices. That is, one can think of them as a transformation rule for objects/coordinates in some n-dimensional (vector) space.\n",
    "\n",
    "3. **Geometrical way**: they are a collection of tangent covectors at one point of some manifold, mixed with another collection of linear functionals associated with the same point, but located in dual space (this explanation is rough), even so, this is the most general and precise way to think about tensors.\n",
    "\n",
    "\n",
    "4. **Physical way**: this, in my opinion, is the best way to visualize them, it is like the algebraic one, has the same issues, but still more general and intuitive.\n",
    "\n",
    "![](https://kindsonthegenius.com/blog/wp-content/uploads/2018/11/Simple-2BTutorials-2Bon-2BTensors.jpg)\n",
    "\n",
    "*To learn more about Tensors, refer this really fun yet informative video tutorial*: https://www.youtube.com/watch?v=bpG3gqDM80w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigen Values and Eigen Vectors\n",
    "\n",
    "Eigenvectors and eigenvalues live in the heart of the data science field. It’s a must-know topic for anyone who wants to understand machine learning in-depth.\n",
    "\n",
    "*Eigenvalues and eigenvectors form the basics of computing and mathematics. They are heavily used by scientists.*\n",
    "\n",
    "The below explanations are going to be from this very nicely written article by Farhad Malik on Medium: https://medium.com/fintechexplained/what-are-eigenvalues-and-eigenvectors-a-must-know-concept-for-machine-learning-80d0fd330e47\n",
    "\n",
    "## 1. Eigenvectors and Eigenvalues Introduction\n",
    "\n",
    "### 1.1 What Is An Eigenvector?\n",
    "\n",
    "I would like to explain this concept in a way that we can easily understand it.\n",
    "For the sake of simplicity, let’s consider that we live in a two-dimensional world.\n",
    "\n",
    "* Alex’s house is located at coordinates [10,10] (x=10 and y =10). Let’s refer to it as vector A.\n",
    "* Furthermore, his friend Bob lives in a house with coordinates [20,20] (x=20 and y=20). I will refer to it as vector B.\n",
    "\n",
    "\n",
    "If Alex wants to meet Bob at his place then Alex would have to travel +10 points on the x-axis and +10 points on the y-axis. This movement and direction can be represented as a two-dimensional vector [10,10]. Let’s refer to it as vector C.\n",
    "\n",
    "We can see that vector A to B are related because vector B can be achieved by scaling (multiplying) the vector A by 2. This is because 2 x [10,10] = [20,20]. This is the address of Bob. Vector C also represents the movement for A to reach B.\n",
    "\n",
    "\n",
    "The key to note is that a vector can contain the magnitude and direction of a movement. So far so good!\n",
    "We learned from the introduction above that large set of data can be represented as a matrix and we need to somehow compress the columns of the sparse matrix to speed up our calculations. Plus if we multiply a matrix by a vector then we achieve a new vector. The multiplication of a matrix by a vector is known as transformation matrices.\n",
    "\n",
    "The new vector can be considered to be in two forms:\n",
    "\n",
    "1. Sometimes, the new transformed vector is just a scaled form of the original vector. This means that the new vector can be re-calculated by simply multiplying a scalar (number) to the original vector; just as in the example of vector A and B above.\n",
    "\n",
    "2. And other times, the transformed vector has no direct scalar relationship with the original vector which we used to multiply to the matrix.\n",
    "\n",
    "Therefore, if our input is a large sparse matrix M then we can find a vector o that can replace the matrix M. The criteria is that the product of matrix M and vector o should be the product of vector o and a scalar n:\n",
    "\n",
    "    M * o = n* o\n",
    "    \n",
    "This means that a matrix M and a vector o can be replaced by a scalar n and a vector o.\n",
    "In this instance, o is the eigenvector and n is the eigenvalue and our target is to find o and n.\n",
    "\n",
    "Therefore an eigenvector is a vector that does not change when a transformation is applied to it, except that it becomes a scaled version of the original vector.\n",
    "\n",
    "Eigenvectors can help us calculating an approximation of a large matrix as a smaller vector. There are many other uses which I will explain later on in the article.\n",
    "\n",
    "Eigenvectors are used to make linear transformation understandable. Think of eigenvectors as stretching/compressing an X-Y line chart without changing their direction.\n",
    "\n",
    "### 1.2 What is an Eigenvalue?\n",
    "\n",
    "Eigenvalue— The scalar that is used to transform (stretch) an Eigenvector.\n",
    "\n",
    "## 2. Where are Eigenvectors and Eigenvalues used?\n",
    "\n",
    "There are multiple uses of eigenvalues and eigenvectors:\n",
    "1. Eigenvalues and Eigenvectors have their importance in linear differential equations where you want to find a rate of change or when you want to maintain relationships between two variables.\n",
    "\n",
    "*Think of eigenvalues and eigenvectors as providing summary of a large matrix*\n",
    "\n",
    "2. We can represent a large set of information in a matrix. Performing computations on a large matrix is a very slow process. To elaborate, one of the key methodologies to improve efficiency in computationally intensive tasks is to reduce the dimensions after ensuring most of the key information is maintained. Hence, one eigenvalue and eigenvector are used to capture key information that is stored in a large matrix. This technique can also be used to improve the performance of data churning components.\n",
    "\n",
    "3. Component analysis is one of the key strategies that is utilised to reduce dimension space without losing valuable information. The core of component analysis (PCA) is built on the concept of eigenvalues and eigenvectors. The concept revolves around computing eigenvectors and eigenvalues of the covariance matrix of the features.\n",
    "\n",
    "4. Additionally, eigenvectors and eigenvalues are used in facial recognition techniques such as EigenFaces.\n",
    "\n",
    "5. They are used to reduce dimension space. The technique of Eigenvectors and Eigenvalues are used to compress the data. As mentioned above, many algorithms such as PCA rely on eigenvalues and eigenvectors to reduce the dimensions\n",
    "\n",
    "6. Eigenvalues are also used in regularisation and they can be used to prevent overfitting.\n",
    "\n",
    "*Eigenvectors and eigenvalues are used to reduce noise in data. They can help us improve efficiency in computationally intensive tasks. They also eliminate features that have a strong correlation between them and also help in reducing over-fitting.*\n",
    "\n",
    "7. We can also use eigenvector to rank items in a dataset. They are heavily used in search engines and calculus.\n",
    "\n",
    "Having said that, it can be slow to compute eigenvectors and eigenvalues. The computation is O(n³)\n",
    "\n",
    "It is now apparent that Eigenvalues and Eigenvectors are one of core concepts to understand in data science.\n",
    "\n",
    "## 3. Calculating Eigenvectors and Eigenvalues\n",
    "\n",
    "*Although we don’t have to calculate the Eigenvalues and Eigenvectors by hand every time but it is important to understand the inner workings to be able to confidently use the algorithms.*\n",
    "\n",
    "### 4.1 Let’s understand the steps first\n",
    "\n",
    "Key Concepts: Let’s go over the following bullet points before we calculate Eigenvalues and Eigenvectors\n",
    "\n",
    "Eigenvalues and Eigenvectors have following components:\n",
    "\n",
    "* The eigenvector is an array with n entries where n is the number of rows (or columns) of a square matrix. The eigenvector is represented as x.\n",
    "* Key Note: The direction of an eigenvector does not change when a linear transformation is applied to it.\n",
    "* Therefore, Eigenvector should be a non-null vector\n",
    "* We are required to find a number of values, known as eigenvalues such that\n",
    "    \n",
    "    A * x = lambda * x\n",
    "\n",
    "The above equation states that we need to find eigenvalue (lambda) and eigenvector (x) such that when we multiply a scalar lambda (eigenvalue) to the vector x (eigenvector) then it should equal to the linear transformation of the matrix A once it is scaled by vector x (eigenvector).\n",
    "\n",
    "**Eigenvalues are represented as lambda.**\n",
    "\n",
    "**Keynote**: The above equation should not be invertible.\n",
    "\n",
    "There are two special keywords which we need to understand: Determinant of a matrix and an identity matrix\n",
    "\n",
    "* The determinant of a matrix is a number that is computed from a square matrix. In a nutshell, the diagonal elements are multiplied by each other and then they are subtracted together. We need to ensure that the determinant of the matrix is 0.\n",
    "\n",
    "* Last component: We need an Identity Matrix. An identity square matrix is a matrix that has 1 in diagonal and all of its elements are 0. The identity matrix is represented as I:\n",
    "\n",
    "![](https://miro.medium.com/max/55/1*LMvVzGqj7hJJv-Fyna7WqQ.png)\n",
    "\n",
    "We can represent\n",
    "\n",
    "    A * x = lambda * x\n",
    "    As:\n",
    "    A * x - lambda * x = 0\n",
    "    \n",
    "Now, we need to compute a characteristic equation.\n",
    "\n",
    "    |A - Lambda * I| = 0\n",
    "    \n",
    "Subsequently\n",
    "\n",
    "    Determinant(A - Lambda * I) = 0\n",
    "\n",
    "### 4.2 How do I calculate Eigenvalue?\n",
    "\n",
    "The task is to find Eigenvalues of size n for a matrix A of size n.\n",
    "\n",
    "Therefore, the aim is to find: Eigenvector and Eigenvalues of A such that:\n",
    "\n",
    "    A * Eigenvector — Eigenvalue * EigenVector = 0\n",
    "\n",
    "Find Lambda Such that Determinant(A — Lambda * I) = 0\n",
    "\n",
    "Based on the concepts learned above:\n",
    "\n",
    "1. lambda * I is:\n",
    "![](https://miro.medium.com/max/304/1*bIQMbqKHI5OM_wnGsnq9bw.png)\n",
    "\n",
    "If A is:\n",
    "![](https://miro.medium.com/max/107/1*KcNyn2BW__twG3ITnjj0-w.png)\n",
    "\n",
    "2. Then A — lambda * I is:\n",
    "![](https://miro.medium.com/max/546/1*OpsOYzopms_iwU8ESsdXTg.png)\n",
    "\n",
    "3. Finally calculate the determinant of (A-lambda*I) as:\n",
    "![](https://miro.medium.com/max/617/1*EGGlvpfv4ICf1IVyOMlr9A.png)\n",
    "\n",
    "Once we solve the equation above, we will get the values of lambda. These values are the Eigenvalues.\n",
    "\n",
    "I will present a working example below to illustrate the theory so that we understand the concepts thoroughly.\n",
    "\n",
    "### 4.3 How do I calculate Eigenvector?\n",
    "\n",
    "Once we have calculated eigenvalues, we can calculate the Eigenvectors of matrix A by using Gaussian Elimination. Gaussian elimination is about converting the matrix to row echelon form. Finally, it is about solving the linear system by back substitution.\n",
    "\n",
    "A detailed explanation of Gaussian elimination is out of the scope of this article so that we can concentrate on Eigenvalues and Eigenvectors.\n",
    "\n",
    "Once we have the Eigenvalues, we can find Eigenvector for each of the Eigenvalues. We can substitute the eigenvalue in the lambda and we will achieve an eigenvector.\n",
    "\n",
    "    (A - lambda * I) * x = 0\n",
    "\n",
    "*Therefore if a square matrix has a size n then we will get n eigenvalues and as a result, n eigenvectors will be computed to represent the matrix.*\n",
    "\n",
    "*Now that we have the key, it is the time to compute the Eigenvalues and Eigenvectors together.*\n",
    "\n",
    "**We will later implement and calculate EigenValues and EigenVectors in Python as we learn python.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference and Links to follow:\n",
    "\n",
    "To learn about Bayesian Statistics, i would suggest to follow this article on AnalyticsVidhya: https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/\n",
    "\n",
    "Boost your linear algebra skills: https://towardsdatascience.com/boost-your-data-sciences-skills-learn-linear-algebra-2c30fdd008cf\n",
    "\n",
    "A comprehensive beginners guide to Linear Algebra for Data Scientists - https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/\n",
    "\n",
    "\n",
    "### Video Tutorials to learn Linear Algebra:\n",
    "\n",
    "Khan Academy has really good tutorials to learn the basics of linear algebra: https://www.khanacademy.org/math/linear-algebra\n",
    "\n",
    "Personally, i would recommed this playlist by 3Blue1Brown (You probably wouldn't find anyone who explains it better): https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
